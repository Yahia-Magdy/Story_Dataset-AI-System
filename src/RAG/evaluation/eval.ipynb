{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cfb90ee",
   "metadata": {},
   "source": [
    "## Using LLM as a Judge for RAG system evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad77932c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd7e1e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast inference is critical for reasoning models because it enables these models to make decisions quickly and efficiently, which is essential for many real-world applications. Here are some reasons why fast inference is crucial for reasoning models:\n",
      "\n",
      "1. **Real-time Decision-Making**: Many applications, such as self-driving cars, robotics, and video games, require rapid decision-making to respond to changing situations. Fast inference allows reasoning models to make decisions in real-time, which is critical for these applications.\n",
      "2. **Low Latency**: In many applications, low latency is essential to provide a responsive and interactive experience. Fast inference helps to reduce the latency between input and output, making the system more responsive and user-friendly.\n",
      "3. **Scalability**: As the number of users or inputs increases, the system needs to be able to handle the increased load without a significant decrease in performance. Fast inference enables reasoning models to scale more efficiently, handling large volumes of data and user requests without compromising performance.\n",
      "4. **Energy Efficiency**: Fast inference can help reduce energy consumption, which is critical for battery-powered devices, such as smartphones, laptops, and robots. By reducing the time it takes to make decisions, fast inference can help minimize energy consumption and prolong battery life.\n",
      "5. **Competitiveness**: In many applications, such as gaming, finance, and autonomous vehicles, speed is a competitive advantage. Fast inference enables reasoning models to make decisions quickly, which can be a key differentiator in these competitive environments.\n",
      "6. **Streaming Data**: In applications that involve streaming data, such as video analysis or sensor data processing, fast inference is essential to keep up with the data flow. Reasoning models need to be able to process and analyze the data in real-time to extract insights and make decisions.\n",
      "7. **Multi-Tasking**: Fast inference enables reasoning models to handle multiple tasks simultaneously, which is essential in many applications, such as smart homes, autonomous vehicles, and robotics.\n",
      "8. **Reducing Memory Requirements**: Fast inference can help reduce memory requirements, as the model can process and analyze data more quickly, reducing the need for large buffers and memory storage.\n",
      "\n",
      "To achieve fast inference, various techniques can be employed, such as:\n",
      "\n",
      "1. **Model pruning**: Reducing the complexity of the model by removing unnecessary weights and connections.\n",
      "2. **Quantization**: Representing model weights and activations using lower-precision data types, such as integers or floating-point numbers.\n",
      "3. **Knowledge distillation**: Transferring knowledge from a large, complex model to a smaller, simpler model.\n",
      "4. **Parallelization**: Using parallel processing techniques, such as GPU acceleration or distributed computing, to speed up inference.\n",
      "5. **Optimized inference algorithms**: Developing custom inference algorithms that are optimized for specific hardware or applications.\n",
      "\n",
      "By achieving fast inference, reasoning models can provide faster, more efficient, and more scalable decision-making capabilities, which is critical for many real-world applications.\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain why fast inference is critical for reasoning models\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8a09eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src folder to sys.path\n",
    "src_path = Path(r\"D:\\Study and Planning\\Projects\\Story Retrieval and Classification System\\src\")\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b2bb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create system + user messages for the evaluator\n",
    "def generate_eval_messages(question, rag_answer, gt_answer):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an evaluator for retrieved-augmented-generation (RAG) answers. \"\n",
    "                \"Your task is to score the RAG model's answers based on correctness, completeness, \"\n",
    "                \"and relevance compared to the query itself and the ground truth answer .\\n\\n\"\n",
    "                \"Scoring Guidelines:\\n\"\n",
    "                \"1. Score from 1 to 5:\\n\"\n",
    "                \"   - 5: Completely correct, fully matches or explains the ground truth.\\n\"\n",
    "                \"   - 4: Mostly correct, minor details missing or slightly inaccurate.\\n\"\n",
    "                \"   - 3: Partially correct, some important points missing.\\n\"\n",
    "                \"   - 2: Mostly incorrect, significant errors.\\n\"\n",
    "                \"   - 1: Completely incorrect or irrelevant.\\n\"\n",
    "                \"2. Provide a short reasoning (1-2 sentences) explaining the score.\\n\"\n",
    "                \"3. ALWAYS output ONLY in JSON format:\\n\"\n",
    "                '{\"score\": <number>, \"reasoning\": \"<your short reasoning>\"}'\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "RAG Answer:\n",
    "{rag_answer}\n",
    "\n",
    "Ground Truth:\n",
    "{gt_answer}\n",
    "\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf5476b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Load your JSON file\n",
    "with open(\"evaluation_data.json\", \"r\") as f:\n",
    "    eval_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20f3b27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete! Results saved to eval_results.json\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for entry in eval_data:\n",
    "    question = entry[\"question\"]\n",
    "    rag_answer = entry[\"rag_answer\"]\n",
    "    gt_answer = entry[\"GT_answer\"]\n",
    "\n",
    "    messages = generate_eval_messages(question, rag_answer, gt_answer)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=messages,\n",
    "        max_tokens=150\n",
    "    )\n",
    "\n",
    "    result_text = completion.choices[0].message.content\n",
    "    results.append({\n",
    "        \"question\": question,\n",
    "        \"rag_answer\": rag_answer,\n",
    "        \"GT_answer\": gt_answer,\n",
    "        \"evaluation\": result_text\n",
    "    })\n",
    "\n",
    "# Save the results to a new JSON file\n",
    "with open(\"eval_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Evaluation complete! Results saved to eval_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc6e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RAG.stores.llm.providers import quantized_model\n",
    "def random_prompt(length_tokens: int):\n",
    "    # Just generates a string with \"word\" repeated\n",
    "    words = [\"token{}\".format(random.randint(0, 999)) for _ in range(length_tokens)]\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8889b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = random_prompt(200)  # 200 tokens input\n",
    "start_time = time.time()\n",
    "\n",
    "llm = quantized_model()\n",
    "answer = llm.run_chat(messages=prompt)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "latency = end_time - start_time\n",
    "print(f\"Latency: {latency:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39622e48",
   "metadata": {},
   "source": [
    "### Calculting the latency and generation speed for the 4 bit quantized QWEN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b68a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "latencies = []\n",
    "tokens_per_second_list = []\n",
    "\n",
    "for _ in range(10):\n",
    "    prompt = random_prompt(200)  # 200 tokens\n",
    "    num_tokens = len(prompt.split())  # count tokens\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = llm.run_chat(prompt)\n",
    "    end_time = time.time()\n",
    "\n",
    "    latency = end_time - start_time\n",
    "    latencies.append(latency)\n",
    "\n",
    "    speed = num_tokens / latency  # tokens per second\n",
    "    tokens_per_second_list.append(speed)\n",
    "\n",
    "avg_latency = sum(latencies) / len(latencies)\n",
    "avg_speed = sum(tokens_per_second_list) / len(tokens_per_second_list)\n",
    "\n",
    "print(f\"Average Latency: {avg_latency:.4f} seconds\")\n",
    "print(f\"Average Generation Speed: {avg_speed:.2f} tokens/sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ada9d9",
   "metadata": {},
   "source": [
    "### Calculting the latency and generation speed for the 8 bit quantized QWEN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4acbea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from RAG.stores.llm.providers import quantized_model\n",
    "llm = quantized_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051bdfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "latencies = []\n",
    "tokens_per_second_list = []\n",
    "\n",
    "for _ in range(10):\n",
    "    prompt = random_prompt(200)  # 200 tokens\n",
    "    num_tokens = len(prompt.split())  # count tokens\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = llm.run_chat(prompt)\n",
    "    end_time = time.time()\n",
    "\n",
    "    latency = end_time - start_time\n",
    "    latencies.append(latency)\n",
    "\n",
    "    speed = num_tokens / latency  # tokens per second\n",
    "    tokens_per_second_list.append(speed)\n",
    "\n",
    "avg_latency = sum(latencies) / len(latencies)\n",
    "avg_speed = sum(tokens_per_second_list) / len(tokens_per_second_list)\n",
    "\n",
    "print(f\"Average Latency: {avg_latency:.4f} seconds\")\n",
    "print(f\"Average Generation Speed: {avg_speed:.2f} tokens/sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f1ee73",
   "metadata": {},
   "source": [
    "### Calculting the latency and generation speed for the main fp16 QWEN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from RAG.stores.llm.providers import GenerativeModel\n",
    "llm = GenerativeModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c0576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "latencies = []\n",
    "tokens_per_second_list = []\n",
    "\n",
    "for _ in range(10):\n",
    "    prompt = random_prompt(200)  # 200 tokens\n",
    "    num_tokens = len(prompt.split())  # count tokens\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = llm.run_chat(prompt)\n",
    "    end_time = time.time()\n",
    "\n",
    "    latency = end_time - start_time\n",
    "    latencies.append(latency)\n",
    "\n",
    "    speed = num_tokens / latency  # tokens per second\n",
    "    tokens_per_second_list.append(speed)\n",
    "\n",
    "avg_latency = sum(latencies) / len(latencies)\n",
    "avg_speed = sum(tokens_per_second_list) / len(tokens_per_second_list)\n",
    "\n",
    "print(f\"Average Latency: {avg_latency:.4f} seconds\")\n",
    "print(f\"Average Generation Speed: {avg_speed:.2f} tokens/sec\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StoryRetrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
